# Agente Analista Financiero - RSS Scraper con Base de Datos

Sistema completo para scrapear feeds RSS financieros, almacenar artÃ­culos en base de datos y generar reportes diarios sobre MSCI World y otros Ã­ndices financieros.

## ğŸš€ CaracterÃ­sticas

- **Scraping de RSS Feeds**: RecolecciÃ³n automÃ¡tica de artÃ­culos de mÃºltiples fuentes RSS
- **Almacenamiento en Base de Datos**: Persistencia de artÃ­culos usando SQLAlchemy (SQLite/PostgreSQL)
- **Filtrado por Fecha**: RecolecciÃ³n de artÃ­culos por rango de fechas o fecha especÃ­fica
- **Contenido Markdown**: Descarga y conversiÃ³n automÃ¡tica del contenido completo de cada artÃ­culo
- **Estructura Modular**: CÃ³digo organizado en clases y mÃ³dulos reutilizables
- **ConfiguraciÃ³n Flexible**: Soporte para SQLite (por defecto) o PostgreSQL mediante variables de entorno

## ğŸ“ Estructura del Proyecto

```
AGENTE-ANALISTA-FINANCIERO-LANGGRAPH/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ database/
â”‚   â”‚   â”œâ”€â”€ connection.py      # GestiÃ³n centralizada de conexiÃ³n a BD
â”‚   â”‚   â”œâ”€â”€ models.py          # Modelos SQLAlchemy (Article, Collection, Extraction)
â”‚   â”‚   â”œâ”€â”€ db_manager.py      # Operaciones de base de datos
â”‚   â”‚   â””â”€â”€ README.md          # DocumentaciÃ³n de la base de datos
â”‚   â””â”€â”€ scrapers/
â”‚       â””â”€â”€ rss_scraper.py     # Clases RSSFetcher y Scraper
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.json            # URLs de feeds RSS
â”œâ”€â”€ examples/
â”‚   â””â”€â”€ database_example.py    # Ejemplo de uso de la base de datos
â”œâ”€â”€ docker/
â”‚   â””â”€â”€ docker-compose.yml     # ConfiguraciÃ³n Docker (opcional)
â”œâ”€â”€ run_scraper.py             # Script principal unificado
â”œâ”€â”€ requirements.txt           # Dependencias del proyecto
â””â”€â”€ README.md                  # Este archivo
```

## ğŸ”§ InstalaciÃ³n

### Requisitos

- Python 3.8+
- pip

### Pasos

1. **Clonar el repositorio** (o descargar el cÃ³digo)

2. **Instalar dependencias**:
```bash
pip install -r requirements.txt
```

3. **Configurar variables de entorno** (opcional):
```bash
# Crear archivo .env en la raÃ­z del proyecto
# Para PostgreSQL (opcional):
POSTGRES_USER=your_user
POSTGRES_PASSWORD=your_password
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=your_database

# Para OpenAI (si usas el agente):
OPENAI_API_KEY=your_openai_key
```

**Nota**: Si no configuras PostgreSQL, el sistema usarÃ¡ SQLite automÃ¡ticamente.

## âš™ï¸ ConfiguraciÃ³n

### 1. Configurar Feeds RSS

Edita `config/config.json` para aÃ±adir o modificar feeds:

```json
{
  "RSS_URLS": {
    "MSCI_WORLD_NEWS_ULR": "https://ir.msci.com/rss/news-releases.xml",
    "SP500_DAILY_INSIGHTS": "https://www.spglobal.com/spdji/en/rss",
    "NASDAQ_NEWS_URL": "https://www.nasdaq.com/feed/nasdaq-original/rss.xml"
  }
}
```

### 2. Base de Datos

El sistema gestiona automÃ¡ticamente la conexiÃ³n:

- **SQLite (por defecto)**: No requiere configuraciÃ³n. Crea `rss_articles.db` automÃ¡ticamente.
- **PostgreSQL**: Configura las variables de entorno en `.env` (ver secciÃ³n InstalaciÃ³n).

## ğŸ“– Uso

### Uso BÃ¡sico - Script Unificado

El script `run_scraper.py` ejecuta todo el pipeline:

```bash
python run_scraper.py
```

Este script:
1. Crea las tablas de base de datos
2. Obtiene los feeds RSS configurados
3. Scrapea artÃ­culos de los Ãºltimos 7 dÃ­as (configurable)
4. Descarga el contenido markdown de cada artÃ­culo
5. Inserta todo en la base de datos
6. Muestra un resumen

### Uso Avanzado - ProgramÃ¡tico

```python
from app.scrapers.rss_scraper import RSSFetcher, Scraper
from app.database.db_manager import DatabaseManager
from datetime import date, timedelta

# 1. Inicializar base de datos
db_manager = DatabaseManager()
db_manager.create_tables()

# 2. Obtener feeds
fetcher = RSSFetcher(config_path="config/config.json")
fetcher.fetch_all()

# 3. Scrapear artÃ­culos
scraper = Scraper(fetcher)

# OpciÃ³n A: Ãšltimos 7 dÃ­as
today = date.today()
week_ago = today - timedelta(days=7)
result = scraper.collect_date_range(start_date=week_ago, end_date=today)

# OpciÃ³n B: Solo hoy
result = scraper.collect_all(filter_date=today)

# OpciÃ³n C: Todos los artÃ­culos
result = scraper.collect_all()

# 4. Guardar en base de datos
extraction_id = scraper.save_to_database()
print(f"Inserted extraction ID: {extraction_id}")

# 5. Consultar base de datos
collections = db_manager.get_collections()
for col in collections:
    print(f"{col['source']}: {col['article_count']} articles")
```

### Filtrado por Fecha

```python
from datetime import date, timedelta

# Filtrar por fecha especÃ­fica
today = date.today()
result = scraper.collect_all(filter_date=today)

# Filtrar por rango
start = date(2025, 11, 1)
end = date(2025, 11, 30)
result = scraper.collect_date_range(start_date=start, end_date=end)

# Filtrar un feed especÃ­fico
collection = scraper.collect_feed("MSCI_WORLD_NEWS_ULR", filter_date=today)
```

### Consultar Base de Datos

```python
from app.database.db_manager import DatabaseManager

db = DatabaseManager()

# Obtener todas las colecciones
collections = db.get_collections()
for col in collections:
    print(f"{col['source']}: {col['article_count']} articles")

# Obtener artÃ­culos por fuente
articles = db.get_articles_by_source("MSCI_WORLD_NEWS_ULR")
for article in articles:
    print(f"{article.title} - {article.link}")

# Obtener todos los artÃ­culos (con lÃ­mite)
articles = db.get_all_articles(limit=10)
```

## ğŸ—„ï¸ Modelos de Base de Datos

### Article
- `id`: ID Ãºnico
- `title`: TÃ­tulo del artÃ­culo
- `source`: Nombre de la fuente RSS
- `link`: URL del artÃ­culo (Ãºnico)
- `published`: Fecha de publicaciÃ³n
- `content`: Contenido en markdown
- `collection_id`: Foreign key a Collection
- `created_at`: Timestamp de inserciÃ³n

### Collection
- `id`: ID Ãºnico
- `source`: Nombre de la fuente (Ãºnico)
- `extraction_id`: Foreign key a Extraction (opcional)
- `created_at`: Timestamp de creaciÃ³n
- `updated_at`: Timestamp de actualizaciÃ³n

### Extraction
- `id`: ID Ãºnico
- `created_at`: Timestamp de la extracciÃ³n

## ğŸ”„ Flujo de Datos

```
RSS Feeds â†’ RSSFetcher â†’ Scraper â†’ Extraction (TypedDict)
                                      â†“
                              DatabaseManager â†’ SQLAlchemy Models
                                      â†“
                                 Database (SQLite/PostgreSQL)
```

## ğŸ“Š Ejemplos

Ver `examples/database_example.py` para un ejemplo completo de uso.

## ğŸ” Variables de Entorno

### Base de Datos (PostgreSQL - Opcional)
- `POSTGRES_USER`: Usuario de PostgreSQL
- `POSTGRES_PASSWORD`: ContraseÃ±a
- `POSTGRES_HOST`: Host (default: localhost)
- `POSTGRES_PORT`: Puerto (default: 5432)
- `POSTGRES_DB`: Nombre de la base de datos

### OpenAI (Opcional - para el agente)
- `OPENAI_API_KEY`: Clave de API de OpenAI

## ğŸ¤– Agente con LangGraph (Futuro)

El proyecto incluye estructura para un agente LangGraph que:
- Sintetiza noticias en lenguaje "for dummies"
- Genera conclusiones accionables
- EnvÃ­a reportes por email

Ver `app/agent/agent.py` para mÃ¡s detalles (si estÃ¡ implementado).

## ğŸ³ Docker (Opcional)

Si usas PostgreSQL, puedes usar Docker:

```bash
cd docker
docker-compose up -d
```

## ğŸ“ Notas

- **SQLite por defecto**: Si no configuras PostgreSQL, el sistema usa SQLite automÃ¡ticamente
- **DetecciÃ³n automÃ¡tica**: El sistema detecta si `psycopg2` estÃ¡ instalado y ajusta el comportamiento
- **Sin duplicados**: Los artÃ­culos se identifican por su `link` Ãºnico, evitando duplicados
- **Filtrado inteligente**: El sistema parsea fechas de mÃºltiples formatos RSS

## ğŸ› ï¸ Troubleshooting

### Error: "PostgreSQL driver not found"
- **SoluciÃ³n**: El sistema automÃ¡ticamente usa SQLite como fallback
- **Para usar PostgreSQL**: `pip install psycopg2-binary`

### Error: "Could not determine join condition"
- **SoluciÃ³n**: AsegÃºrate de recrear las tablas ejecutando `db_manager.create_tables()`

### Feeds no se cargan
- Verifica que las URLs en `config/config.json` sean vÃ¡lidas
- Algunos feeds pueden requerir headers especÃ­ficos (ya implementados)

## ğŸ“„ Licencia

[Especificar licencia si aplica]

## ğŸ¤ Contribuciones

[Instrucciones de contribuciÃ³n si aplica]
